@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{WinNT,
  author = "{R. Evtimov, M. Falli, A. Maiwald}",
  title = {BIDIRECTIONAL ENCODER REPRESENTATIONS FROM TRANSFORMERS (BERT)},
  howpublished = {\url{https://humboldt-wi.github.io/blog/research/information_systems_1920/bert_blog_post/}},
  year = 2020
}

@article{mccormick2019bert,
  title={Bert fine-tuning tutorial with pytorch},
  author={McCormick, Chris and Ryan, Nick},
  journal={Retrieved January},
  volume={24},
  pages={2021},
  year={2019}
}

@misc{QAwithBERT,
  title = {Question Answering with a Fine-Tuned BERT},
  howpublished = {\url{https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/?fbclid=IwAR2mVPIS4-H0EstiOMzB7AG-ywvH8VqYXs7bwyrALdbUQFYMy-LFOtjXq0Q}},
  note = {Accessed: 2020-03-10}
}

@misc{BERTmodel,
  title = {BERTModelContent},
  howpublished = {\url{https://phamdinhkhanh.github.io/2020/05/23/BERTModel.html}},
  note = {Accessed: 2020-03-10}
}