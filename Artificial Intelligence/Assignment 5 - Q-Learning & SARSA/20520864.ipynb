{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20520864.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8P_laMcSQNk"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "from IPython import display\n",
        "from prettytable import PrettyTable"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGopsD0IWpDO"
      },
      "source": [
        "def play(env, q_table, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state, :])\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, policy, evaluatate_episode):\n",
        "    num_of_success = 0\n",
        "    succeed_steps = []\n",
        "    succeed_rewards = [] \n",
        "    for i in range(evaluatate_episode):\n",
        "        total_reward, steps = play(env, policy)\n",
        "        if total_reward > 0:\n",
        "            succeed_rewards.append(total_reward)\n",
        "            num_of_success += 1\n",
        "            succeed_steps.append(steps)\n",
        "\n",
        "    success = f'{num_of_success}/{evaluatate_episode}'\n",
        "    mean_succeed_steps = None\n",
        "    mean_succeed_reward = None\n",
        "    if len(succeed_steps) > 0:\n",
        "      mean_succeed_reward = np.mean(succeed_rewards)\n",
        "      mean_succeed_steps = np.mean(succeed_steps)\n",
        "\n",
        "    return success, mean_succeed_reward, mean_succeed_steps"
      ],
      "metadata": {
        "id": "2l8BKi9TSqRe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3xVez-WTeww"
      },
      "source": [
        "def q_learning(env, train_episode, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    def policy(state):\n",
        "      nonlocal epsilon, env, q_table\n",
        "      exploration = random.uniform(0,1)\n",
        "      if exploration < epsilon:\n",
        "          action = env.action_space.sample()\n",
        "      else:\n",
        "          action = np.argmax(q_table[state, :])\n",
        "      return action\n",
        "\n",
        "    # q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    q_table = np.random.rand(env.observation_space.n, env.action_space.n)\n",
        "    rewards_all = []\n",
        "    for episode in range(train_episode):\n",
        "        state = env.reset()\n",
        "\n",
        "        reward_episode = 0.0\n",
        "        done = False\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        for step in range(num_steps_per_episode):\n",
        "            action = policy(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * np.max(q_table[next_state,:]))\n",
        "\n",
        "            reward_episode += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        rewards_all.append(reward_episode)\n",
        "    return q_table, rewards_all"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, train_episode, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "    def policy(state):\n",
        "      nonlocal epsilon, env, q_table\n",
        "      exploration = random.uniform(0,1)\n",
        "      if exploration < epsilon:\n",
        "          action = env.action_space.sample()\n",
        "      else:\n",
        "          action = np.argmax(q_table[state, :])\n",
        "      return action\n",
        "\n",
        "    # q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    q_table = np.random.rand(env.observation_space.n, env.action_space.n)\n",
        "    rewards_all = []\n",
        "    for episode in range(train_episode):\n",
        "\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate*episode)\n",
        "        state0 = env.reset()\n",
        "        action0 = policy(state0)\n",
        "        state1, reward1, done, info = env.step(action0)\n",
        "        reward_episode = reward1\n",
        "\n",
        "        for step in range(1, num_steps_per_episode):\n",
        "            if done:\n",
        "                break\n",
        "            action1 = policy(state1)\n",
        "            state2, reward2, done, info = env.step(action1)\n",
        "            reward_episode += reward2\n",
        "\n",
        "            q_table[state0, action0] = q_table[state0, action0] * (1 - learning_rate) + learning_rate * (reward1 + gamma * (q_table[state1, action1]))\n",
        "\n",
        "            state0 = state1\n",
        "            state1 = state2\n",
        "            reward1 = reward2\n",
        "            action0 = action1\n",
        "\n",
        "        rewards_all.append(reward_episode)\n",
        "    return q_table, rewards_all"
      ],
      "metadata": {
        "id": "dPyTMVg09zV9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xifGZ8j-SWPT"
      },
      "source": [
        "def compare(toy_game, evaluatate_episode, train_episode, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate):\n",
        "  def run_algo(algo):\n",
        "    nonlocal env, table, row\n",
        "    \n",
        "    start = time.time()\n",
        "    q_table, rewards_all = algo(env, train_episode, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "    end = time.time()\n",
        "    learning_seconds = end - start\n",
        "    mean_reward_on_learning = np.mean(rewards_all)\n",
        "\n",
        "    row += [learning_seconds, mean_reward_on_learning] + list(play_multiple_times(env, q_table, evaluatate_episode))\n",
        "    for i in range(len(row)):\n",
        "      if type(row[i]) is float or type(row[i]) is np.float64:\n",
        "        row[i] = round(row[i], 4)\n",
        "    table.add_row(row)\n",
        "\n",
        "  env = gym.make(toy_game)\n",
        "  table = PrettyTable(['algo', 'learning_seconds', 'mean_reward_on_learning', 'success', 'mean_succeed_reward', 'mean_succeed_steps'])\n",
        "\n",
        "  row = ['Q-Learning']\n",
        "  run_algo(q_learning)\n",
        "\n",
        "  row = ['SARSA']\n",
        "  run_algo(sarsa)\n",
        "\n",
        "  print(f'{toy_game}: {env.observation_space.n}x{env.action_space.n}')\n",
        "  print(table)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.1\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "epsilon_decay_rate = 0.005\n",
        "\n",
        "num_steps_per_episode = 100\n",
        "train_episode = 20000\n",
        "evaluatate_episode = 1000\n",
        "# train_episode = 1000\n",
        "# evaluatate_episode = 100\n",
        "\n",
        "toy_games = [\"FrozenLake-v0\", \"FrozenLake8x8-v0\", \"Taxi-v3\"]\n",
        "\n",
        "for toy_game in toy_games:\n",
        "  compare(toy_game, evaluatate_episode, train_episode, num_steps_per_episode, learning_rate, gamma, max_epsilon, min_epsilon, epsilon_decay_rate)\n",
        "  print()"
      ],
      "metadata": {
        "id": "bm4CcsAzSx-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e85d3c0-c5b3-4a7c-e564-bb01c7cff7b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FrozenLake-v0: 16x4\n",
            "+------------+------------------+-------------------------+----------+---------------------+--------------------+\n",
            "|    algo    | learning_seconds | mean_reward_on_learning | success  | mean_succeed_reward | mean_succeed_steps |\n",
            "+------------+------------------+-------------------------+----------+---------------------+--------------------+\n",
            "| Q-Learning |     16.5599      |          0.5096         | 607/1000 |         1.0         |      33.8764       |\n",
            "|   SARSA    |      2.1185      |          0.0231         | 30/1000  |         1.0         |      11.6667       |\n",
            "+------------+------------------+-------------------------+----------+---------------------+--------------------+\n",
            "\n",
            "FrozenLake8x8-v0: 64x4\n",
            "+------------+------------------+-------------------------+---------+---------------------+--------------------+\n",
            "|    algo    | learning_seconds | mean_reward_on_learning | success | mean_succeed_reward | mean_succeed_steps |\n",
            "+------------+------------------+-------------------------+---------+---------------------+--------------------+\n",
            "| Q-Learning |      8.0653      |          0.0001         |  0/1000 |         None        |        None        |\n",
            "|   SARSA    |      5.1707      |          0.0008         |  0/1000 |         None        |        None        |\n",
            "+------------+------------------+-------------------------+---------+---------------------+--------------------+\n",
            "\n",
            "Taxi-v3: 500x6\n",
            "+------------+------------------+-------------------------+-----------+---------------------+--------------------+\n",
            "|    algo    | learning_seconds | mean_reward_on_learning |  success  | mean_succeed_reward | mean_succeed_steps |\n",
            "+------------+------------------+-------------------------+-----------+---------------------+--------------------+\n",
            "| Q-Learning |      9.3346      |          0.0884         | 1000/1000 |        7.782        |       13.218       |\n",
            "|   SARSA    |      7.1327      |         -5.0828         |  967/1000 |        8.0238       |      12.9762       |\n",
            "+------------+------------------+-------------------------+-----------+---------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhận xét\n",
        "  # SARSA học nhanh hơn Q-Learning\n",
        "  # Phần thưởng khi học của SARSA ít hơn Q-Learning\n",
        "  # Ở các ván thành công, SARSA cho số bước ít hơn và phần thưởng nhiều hơn Q-Learning\n",
        "  # Khả năng hoàn thành ván chơi thành công cửa SARSA ít hơn Q-Learning"
      ],
      "metadata": {
        "id": "KJxtM2E3Powr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FrozenLake-v0: 16x4\n",
        "# +------------+------------------+-------------------------+----------+---------------------+--------------------+\n",
        "# |    algo    | learning_seconds | mean_reward_on_learning | success  | mean_succeed_reward | mean_succeed_steps |\n",
        "# +------------+------------------+-------------------------+----------+---------------------+--------------------+\n",
        "# | Q-Learning |     16.5599      |          0.5096         | 607/1000 |         1.0         |      33.8764       |\n",
        "# |   SARSA    |      2.1185      |          0.0231         | 30/1000  |         1.0         |      11.6667       |\n",
        "# +------------+------------------+-------------------------+----------+---------------------+--------------------+\n",
        "\n",
        "# FrozenLake8x8-v0: 64x4\n",
        "# +------------+------------------+-------------------------+---------+---------------------+--------------------+\n",
        "# |    algo    | learning_seconds | mean_reward_on_learning | success | mean_succeed_reward | mean_succeed_steps |\n",
        "# +------------+------------------+-------------------------+---------+---------------------+--------------------+\n",
        "# | Q-Learning |      8.0653      |          0.0001         |  0/1000 |         None        |        None        |\n",
        "# |   SARSA    |      5.1707      |          0.0008         |  0/1000 |         None        |        None        |\n",
        "# +------------+------------------+-------------------------+---------+---------------------+--------------------+\n",
        "\n",
        "# Taxi-v3: 500x6\n",
        "# +------------+------------------+-------------------------+-----------+---------------------+--------------------+\n",
        "# |    algo    | learning_seconds | mean_reward_on_learning |  success  | mean_succeed_reward | mean_succeed_steps |\n",
        "# +------------+------------------+-------------------------+-----------+---------------------+--------------------+\n",
        "# | Q-Learning |      9.3346      |          0.0884         | 1000/1000 |        7.782        |       13.218       |\n",
        "# |   SARSA    |      7.1327      |         -5.0828         |  967/1000 |        8.0238       |      12.9762       |\n",
        "# +------------+------------------+-------------------------+-----------+---------------------+--------------------+"
      ],
      "metadata": {
        "id": "XOIdwlnUPj7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}