{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20520864.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from prettytable import PrettyTable\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    num_of_success = 0\n",
        "    succeed_steps = []\n",
        "    succeed_rewards = [] \n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "        if total_reward > 0:\n",
        "            succeed_rewards.append(total_reward)\n",
        "            num_of_success += 1\n",
        "            succeed_steps.append(steps)\n",
        "\n",
        "    success = f'{num_of_success}/{max_episodes}'\n",
        "    mean_succeed_steps = None\n",
        "    mean_succeed_reward = None\n",
        "    if len(succeed_steps) > 0:\n",
        "      mean_succeed_reward = np.mean(succeed_rewards)\n",
        "      mean_succeed_steps = np.mean(succeed_steps)\n",
        "\n",
        "    return success, mean_succeed_reward, mean_succeed_steps"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma):\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        policy[state] = np.argmax(q_values)\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters, gamma):\n",
        "    # v_values = np.zeros(env.observation_space.n)\n",
        "    v_values = np.random.rand(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            \n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            v_values[state] = max(q_values)\n",
        "        \n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            converge_at = i\n",
        "            break\n",
        "    \n",
        "    policy = policy_extraction(env, v_values, gamma)\n",
        "    return policy, converge_at"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iters, gamma):\n",
        "    # policy = np.zeros(env.observation_space.n)\n",
        "    policy = np.random.randint(env.action_space.n, size=env.observation_space.n)\n",
        "    # v_values = np.zeros(env.observation_space.n)\n",
        "    v_values = np.random.rand(env.observation_space.n)\n",
        "\n",
        "    equal_before = False\n",
        "    converge_at = None\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "        prev_policy = policy\n",
        "\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = prev_policy[state]\n",
        "            v_values[state] = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                v_values[state] += prob * (reward + gamma * prev_v_values[next_state])\n",
        "            \n",
        "        policy = policy_extraction(env, v_values, gamma)\n",
        "        \n",
        "        if np.array_equal(policy, prev_policy):\n",
        "          if equal_before:\n",
        "            converge_at = i\n",
        "            break\n",
        "          else:\n",
        "            equal_before = True\n",
        "    \n",
        "    return policy, converge_at"
      ],
      "metadata": {
        "id": "ohmyN9a1RUuy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare(toy_game, max_episodes, max_iters, gamma):\n",
        "  def run_algo(algo):\n",
        "    nonlocal env, table, row\n",
        "    \n",
        "    start = time.time()\n",
        "    policy, converge_at = algo(env, max_iters, gamma)\n",
        "    end = time.time()\n",
        "    learning_seconds = end - start\n",
        "\n",
        "    row += [learning_seconds, converge_at] + list(play_multiple_times(env, policy, max_episodes))\n",
        "    for i in range(len(row)):\n",
        "      if type(row[i]) is float or type(row[i]) is np.float64:\n",
        "        row[i] = round(row[i], 4)\n",
        "    table.add_row(row)\n",
        "\n",
        "  env = gym.make(toy_game)\n",
        "  table = PrettyTable(['algo', 'learning_seconds', 'converge_at', 'success', 'mean_succeed_reward', 'mean_succeed_steps'])\n",
        "\n",
        "  row = ['Value Iteration']\n",
        "  run_algo(value_iteration)\n",
        "\n",
        "  row = ['Policy Iteration']\n",
        "  run_algo(policy_iteration)\n",
        "\n",
        "  print(f'{toy_game}: {env.observation_space.n}x{env.action_space.n}')\n",
        "  print(table)"
      ],
      "metadata": {
        "id": "vJ7qCRSASkXv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "max_iters = 500\n",
        "gamma = 0.9\n",
        "\n",
        "max_episodes = 1000\n",
        "\n",
        "toy_games = [\"FrozenLake-v0\", \"FrozenLake8x8-v0\", \"Taxi-v3\"]\n",
        "\n",
        "for toy_game in toy_games:\n",
        "  compare(toy_game, max_episodes, max_iters, gamma)\n",
        "  print()"
      ],
      "metadata": {
        "id": "R_JOE1X6Smrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4550d44e-1722-4a01-acb0-61a7cce107ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FrozenLake-v0: 16x4\n",
            "+------------------+------------------+-------------+----------+---------------------+--------------------+\n",
            "|       algo       | learning_seconds | converge_at | success  | mean_succeed_reward | mean_succeed_steps |\n",
            "+------------------+------------------+-------------+----------+---------------------+--------------------+\n",
            "| Value Iteration  |      0.0324      |     152     | 751/1000 |         1.0         |      37.1039       |\n",
            "| Policy Iteration |      0.0011      |      2      | 369/1000 |         1.0         |      29.7263       |\n",
            "+------------------+------------------+-------------+----------+---------------------+--------------------+\n",
            "\n",
            "FrozenLake8x8-v0: 64x4\n",
            "+------------------+------------------+-------------+----------+---------------------+--------------------+\n",
            "|       algo       | learning_seconds | converge_at | success  | mean_succeed_reward | mean_succeed_steps |\n",
            "+------------------+------------------+-------------+----------+---------------------+--------------------+\n",
            "| Value Iteration  |      0.1263      |     153     | 743/1000 |         1.0         |      71.5007       |\n",
            "| Policy Iteration |       0.05       |      24     | 625/1000 |         1.0         |      69.3872       |\n",
            "+------------------+------------------+-------------+----------+---------------------+--------------------+\n",
            "\n",
            "Taxi-v3: 500x6\n",
            "+------------------+------------------+-------------+-----------+---------------------+--------------------+\n",
            "|       algo       | learning_seconds | converge_at |  success  | mean_succeed_reward | mean_succeed_steps |\n",
            "+------------------+------------------+-------------+-----------+---------------------+--------------------+\n",
            "| Value Iteration  |      0.6865      |     116     | 1000/1000 |         7.95        |       13.05        |\n",
            "| Policy Iteration |      0.2682      |      19     | 1000/1000 |        8.029        |       12.971       |\n",
            "+------------------+------------------+-------------+-----------+---------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nhận xét\n",
        "  # Policy Iteration hội tụ sớm hơn Value Iteration\n",
        "  # Ở các ván thành công, Policy Iteration THƯỜNG cho số bước ít hơn Value Iteration\n",
        "  # Khả năng hoàn thành ván chơi thành công cửa Policy Iteration ít hơn Value Iteration"
      ],
      "metadata": {
        "id": "lzlTOiHsafrw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FrozenLake-v0: 16x4\n",
        "# +------------------+------------------+-------------+----------+---------------------+--------------------+\n",
        "# |       algo       | learning_seconds | converge_at | success  | mean_succeed_reward | mean_succeed_steps |\n",
        "# +------------------+------------------+-------------+----------+---------------------+--------------------+\n",
        "# | Value Iteration  |      0.0324      |     152     | 751/1000 |         1.0         |      37.1039       |\n",
        "# | Policy Iteration |      0.0011      |      2      | 369/1000 |         1.0         |      29.7263       |\n",
        "# +------------------+------------------+-------------+----------+---------------------+--------------------+\n",
        "\n",
        "# FrozenLake8x8-v0: 64x4\n",
        "# +------------------+------------------+-------------+----------+---------------------+--------------------+\n",
        "# |       algo       | learning_seconds | converge_at | success  | mean_succeed_reward | mean_succeed_steps |\n",
        "# +------------------+------------------+-------------+----------+---------------------+--------------------+\n",
        "# | Value Iteration  |      0.1263      |     153     | 743/1000 |         1.0         |      71.5007       |\n",
        "# | Policy Iteration |       0.05       |      24     | 625/1000 |         1.0         |      69.3872       |\n",
        "# +------------------+------------------+-------------+----------+---------------------+--------------------+\n",
        "\n",
        "# Taxi-v3: 500x6\n",
        "# +------------------+------------------+-------------+-----------+---------------------+--------------------+\n",
        "# |       algo       | learning_seconds | converge_at |  success  | mean_succeed_reward | mean_succeed_steps |\n",
        "# +------------------+------------------+-------------+-----------+---------------------+--------------------+\n",
        "# | Value Iteration  |      0.6865      |     116     | 1000/1000 |         7.95        |       13.05        |\n",
        "# | Policy Iteration |      0.2682      |      19     | 1000/1000 |        8.029        |       12.971       |\n",
        "# +------------------+------------------+-------------+-----------+---------------------+--------------------+\n"
      ],
      "metadata": {
        "id": "VPc-9GX7Ullb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}